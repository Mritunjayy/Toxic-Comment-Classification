## We have broken down a large dataset into small chunks of 6 new datasets using 

from google.colab import drive
drive.mount('/content/drive')
!apt-get install git
!git config --global user.name "Mritunjayy"
!git config --global user.email "pmritunjay635@gmail.com"
import os
os.environ['GITHUB_TOKEN'] = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
!git clone https://{os.environ['GITHUB_TOKEN']}@github.com/Mritunjayy/Toxic-Comment-Classification.git
%cd Toxic-Comment-Classification
import pandas as pd
file_path = '/content/train.csv'
df = pd.read_csv(file_path)
chunk_size = 25000  # Adjust based on your needs
for i, chunk in enumerate(range(0, df.shape[0], chunk_size)):
    chunk_path = f'train_{i}.csv'
    df.iloc[chunk:chunk + chunk_size].to_csv(chunk_path, index=False)
    !git add {chunk_path}
    !git commit -m "Add chunk {i} of large CSV file"
!git push origin main
